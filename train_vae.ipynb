{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnYCBuvG2U1E",
        "outputId": "10ca8e4b-1940-4b2c-cc85-cbb8dcfc9c01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting folium==0.2.1\n",
            "  Downloading folium-0.2.1.tar.gz (69 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▊                           | 10 kB 17.4 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 20 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 30 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 40 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 51 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 61 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 69 kB 3.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Jinja2 in /usr/local/lib/python3.7/dist-packages (from folium==0.2.1) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2->folium==0.2.1) (2.0.1)\n",
            "Building wheels for collected packages: folium\n",
            "  Building wheel for folium (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for folium: filename=folium-0.2.1-py3-none-any.whl size=79808 sha256=63aec494a8c23d5995ffad73b8fb932c35e6a9625fab73a0e7b0043472476da9\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/f0/3a/3f79a6914ff5affaf50cabad60c9f4d565283283c97f0bdccf\n",
            "Successfully built folium\n",
            "Installing collected packages: folium\n",
            "  Attempting uninstall: folium\n",
            "    Found existing installation: folium 0.8.3\n",
            "    Uninstalling folium-0.8.3:\n",
            "      Successfully uninstalled folium-0.8.3\n",
            "Successfully installed folium-0.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install folium==0.2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vdCOZwEhrieW",
        "outputId": "fa175052-3ca1-49c7-ee91-e5ab9206f369"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting dalle-pytorch\n",
            "  Downloading dalle_pytorch-1.4.2-py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 2.4 MB/s \n",
            "\u001b[?25hCollecting tokenizers\n",
            "  Downloading tokenizers-0.11.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 40.3 MB/s \n",
            "\u001b[?25hCollecting youtokentome\n",
            "  Downloading youtokentome-1.0.6-cp37-cp37m-manylinux2010_x86_64.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 32.4 MB/s \n",
            "\u001b[?25hCollecting taming-transformers-rom1504\n",
            "  Downloading taming_transformers_rom1504-0.0.6-py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 113 kB/s \n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 29.6 MB/s \n",
            "\u001b[?25hCollecting WebDataset\n",
            "  Downloading webdataset-0.1.103-py3-none-any.whl (47 kB)\n",
            "\u001b[K     |████████████████████████████████| 47 kB 4.1 MB/s \n",
            "\u001b[?25hCollecting axial-positional-embedding\n",
            "  Downloading axial_positional_embedding-0.2.1.tar.gz (2.6 kB)\n",
            "Collecting DALL-E\n",
            "  Downloading DALL_E-0.1-py3-none-any.whl (6.0 kB)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from dalle-pytorch) (1.10.0+cu111)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from dalle-pytorch) (2019.12.20)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from dalle-pytorch) (7.1.2)\n",
            "Collecting rotary-embedding-torch\n",
            "  Downloading rotary_embedding_torch-0.1.2-py3-none-any.whl (4.1 kB)\n",
            "Collecting einops>=0.3.2\n",
            "  Downloading einops-0.4.0-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from dalle-pytorch) (0.11.1+cu111)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from dalle-pytorch) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->dalle-pytorch) (3.10.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from DALL-E->dalle-pytorch) (2.23.0)\n",
            "Collecting mypy\n",
            "  Downloading mypy-0.931-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (14.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.0 MB 33.2 MB/s \n",
            "\u001b[?25hCollecting blobfile\n",
            "  Downloading blobfile-1.2.8-py3-none-any.whl (66 kB)\n",
            "\u001b[K     |████████████████████████████████| 66 kB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from DALL-E->dalle-pytorch) (3.6.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from DALL-E->dalle-pytorch) (1.19.5)\n",
            "Collecting urllib3~=1.25\n",
            "  Downloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 47.3 MB/s \n",
            "\u001b[?25hCollecting pycryptodomex~=3.8\n",
            "  Downloading pycryptodomex-3.14.1-cp35-abi3-manylinux2010_x86_64.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 34.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock~=3.0 in /usr/local/lib/python3.7/dist-packages (from blobfile->DALL-E->dalle-pytorch) (3.4.2)\n",
            "Collecting xmltodict~=0.12.0\n",
            "  Downloading xmltodict-0.12.0-py2.py3-none-any.whl (9.2 kB)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->dalle-pytorch) (0.2.5)\n",
            "Collecting typed-ast<2,>=1.4.0\n",
            "  Downloading typed_ast-1.5.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (843 kB)\n",
            "\u001b[K     |████████████████████████████████| 843 kB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from mypy->DALL-E->dalle-pytorch) (2.0.0)\n",
            "Collecting mypy-extensions>=0.4.3\n",
            "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->DALL-E->dalle-pytorch) (8.12.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->DALL-E->dalle-pytorch) (21.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pytest->DALL-E->dalle-pytorch) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->DALL-E->dalle-pytorch) (57.4.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->DALL-E->dalle-pytorch) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->DALL-E->dalle-pytorch) (1.11.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->DALL-E->dalle-pytorch) (1.4.0)\n",
            "Collecting urllib3~=1.25\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 48.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->DALL-E->dalle-pytorch) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->DALL-E->dalle-pytorch) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->DALL-E->dalle-pytorch) (2.10)\n",
            "Collecting pytorch-lightning>=1.0.8\n",
            "  Downloading pytorch_lightning-1.5.9-py3-none-any.whl (527 kB)\n",
            "\u001b[K     |████████████████████████████████| 527 kB 46.8 MB/s \n",
            "\u001b[?25hCollecting omegaconf>=2.0.0\n",
            "  Downloading omegaconf-2.1.1-py3-none-any.whl (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 3.2 MB/s \n",
            "\u001b[?25hCollecting PyYAML>=5.1.0\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 44.8 MB/s \n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 45.3 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2022.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 43.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.0.8->taming-transformers-rom1504->dalle-pytorch) (2.7.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-59.5.0-py3-none-any.whl (952 kB)\n",
            "\u001b[K     |████████████████████████████████| 952 kB 43.6 MB/s \n",
            "\u001b[?25hCollecting torchmetrics>=0.4.1\n",
            "  Downloading torchmetrics-0.7.1-py3-none-any.whl (397 kB)\n",
            "\u001b[K     |████████████████████████████████| 397 kB 43.5 MB/s \n",
            "\u001b[?25hCollecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 44.9 MB/s \n",
            "\u001b[?25hCollecting pyDeprecate==0.3.1\n",
            "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.0.8->taming-transformers-rom1504->dalle-pytorch) (21.3)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 38.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning>=1.0.8->taming-transformers-rom1504->dalle-pytorch) (3.0.7)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers-rom1504->dalle-pytorch) (1.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers-rom1504->dalle-pytorch) (0.37.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers-rom1504->dalle-pytorch) (3.17.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers-rom1504->dalle-pytorch) (1.35.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers-rom1504->dalle-pytorch) (1.0.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers-rom1504->dalle-pytorch) (1.43.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers-rom1504->dalle-pytorch) (3.3.6)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers-rom1504->dalle-pytorch) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers-rom1504->dalle-pytorch) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers-rom1504->dalle-pytorch) (0.6.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers-rom1504->dalle-pytorch) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers-rom1504->dalle-pytorch) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers-rom1504->dalle-pytorch) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers-rom1504->dalle-pytorch) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers-rom1504->dalle-pytorch) (4.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers-rom1504->dalle-pytorch) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers-rom1504->dalle-pytorch) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers-rom1504->dalle-pytorch) (3.2.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.0.8->taming-transformers-rom1504->dalle-pytorch) (2.0.11)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 49.4 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 32.8 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 4.8 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 47.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->dalle-pytorch) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->dalle-pytorch) (1.1.0)\n",
            "Collecting braceexpand\n",
            "  Downloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n",
            "Building wheels for collected packages: axial-positional-embedding, ftfy, antlr4-python3-runtime, future\n",
            "  Building wheel for axial-positional-embedding (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for axial-positional-embedding: filename=axial_positional_embedding-0.2.1-py3-none-any.whl size=2901 sha256=a861589d63ee6e440e9e315f0f1746af1bd3da998c4b9170e47f17d8a1cee71a\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/2c/c3/9a1cb267c0d0d9b6eeba7952addb32b17857d1f799690c27a8\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41933 sha256=365390269ab8ddfe684da4851e9c61a390d145b39897bc159efa174cf3fefba9\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=f4ac26e30f763ed2482142826cff2658286d7205f37844bcc11feaede3e7e53e\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=a2fa2a59b4776529226cfb5c5d2293b6809f7dff7adb5e4452bfb2682749a2de\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built axial-positional-embedding ftfy antlr4-python3-runtime future\n",
            "Installing collected packages: urllib3, setuptools, multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, pyDeprecate, fsspec, aiohttp, xmltodict, typed-ast, torchmetrics, PyYAML, pycryptodomex, mypy-extensions, future, antlr4-python3-runtime, tokenizers, sacremoses, pytorch-lightning, omegaconf, mypy, huggingface-hub, einops, braceexpand, blobfile, youtokentome, WebDataset, transformers, taming-transformers-rom1504, rotary-embedding-torch, ftfy, DALL-E, axial-positional-embedding, dalle-pytorch\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed DALL-E-0.1 PyYAML-6.0 WebDataset-0.1.103 aiohttp-3.8.1 aiosignal-1.2.0 antlr4-python3-runtime-4.8 async-timeout-4.0.2 asynctest-0.13.0 axial-positional-embedding-0.2.1 blobfile-1.2.8 braceexpand-0.1.7 dalle-pytorch-1.4.2 einops-0.4.0 frozenlist-1.3.0 fsspec-2022.1.0 ftfy-6.0.3 future-0.18.2 huggingface-hub-0.4.0 multidict-6.0.2 mypy-0.931 mypy-extensions-0.4.3 omegaconf-2.1.1 pyDeprecate-0.3.1 pycryptodomex-3.14.1 pytorch-lightning-1.5.9 rotary-embedding-torch-0.1.2 sacremoses-0.0.47 setuptools-59.5.0 taming-transformers-rom1504-0.0.6 tokenizers-0.11.4 torchmetrics-0.7.1 transformers-4.16.2 typed-ast-1.5.2 urllib3-1.25.11 xmltodict-0.12.0 yarl-1.7.2 youtokentome-1.0.6\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources",
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install dalle-pytorch --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKx5cVzbroq1",
        "outputId": "39979c9f-ce5e-4934-c5b9-e298f39ef24d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.12.10-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.4-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[K     |████████████████████████████████| 143 kB 29.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.26-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 20.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Collecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=fae4992f5c94decfa40d390657be8ff7502d34f250db95bf997ce1305ca2a119\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, yaspin, shortuuid, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.26 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.4 shortuuid-1.0.8 smmap-5.0.0 wandb-0.12.10 yaspin-2.1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BARwvLaoWPKq",
        "outputId": "55720e75-ef6c-47b2-9b02-701e36c8ad5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1IsZG5VOtMHXjJ3rvNZ5O2_JoOWXYLGvI\n",
            "To: /content/botpeg.zip\n",
            "\r  0% 0.00/1.56M [00:00<?, ?B/s]\r100% 1.56M/1.56M [00:00<00:00, 159MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown https://drive.google.com/uc?id=1IsZG5VOtMHXjJ3rvNZ5O2_JoOWXYLGvI\n",
        "# https://drive.google.com/uc?id=11bpXgN6rpzhSCPF0KDR6KMNsPanbt3b9\n",
        "# https://drive.google.com/uc?id=1dj1cGLEV0v18uGObxQ2PnN33ayLs3iwq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8Ner3parsps",
        "outputId": "f91bd6e2-806f-4d79-cc52-dfc835ff5f31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  botpeg.zip\n",
            "   creating: botpeg/attributes/\n",
            " extracting: botpeg/attributes/certainties.txt  \n",
            "  inflating: botpeg/attributes/class_attribute_labels_continuous.txt  \n",
            "  inflating: botpeg/attributes/image_attribute_labels.txt  \n",
            "  inflating: botpeg/bounding_boxes.txt  \n",
            "  inflating: botpeg/classes.txt      \n",
            "  inflating: botpeg/image_class_labels.txt  \n",
            "  inflating: botpeg/images.txt       \n",
            "   creating: botpeg/images/\n",
            "   creating: botpeg/images/1.Dance/\n",
            "  inflating: botpeg/images/1.Dance/Dance_01.jpg  \n",
            "  inflating: botpeg/images/1.Dance/Dance_02.jpg  \n",
            "   creating: botpeg/images/10.Singing/\n",
            "  inflating: botpeg/images/10.Singing/Singing_01.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_02.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_03.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_04.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_05.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_06.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_07.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_08.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_09.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_10.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_11.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_12.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_13.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_14.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_15.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_16.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_17.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_18.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_19.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_20.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_21.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_22.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_23.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_24.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_25.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_26.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_27.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_28.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_29.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_30.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_31.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_32.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_33.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_34.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_35.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_36.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_37.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_38.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_39.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_40.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_41.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_42.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_43.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_44.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_45.jpg  \n",
            "  inflating: botpeg/images/10.Singing/Singing_46.jpg  \n",
            "   creating: botpeg/images/11.Thinking/\n",
            "  inflating: botpeg/images/11.Thinking/Thinking_01.jpg  \n",
            "  inflating: botpeg/images/11.Thinking/Thinking_02.jpg  \n",
            "   creating: botpeg/images/12.Winner/\n",
            "  inflating: botpeg/images/12.Winner/Winner.jpg  \n",
            "   creating: botpeg/images/13.Worried/\n",
            "  inflating: botpeg/images/13.Worried/Worried.jpg  \n",
            "   creating: botpeg/images/2.Excited/\n",
            "  inflating: botpeg/images/2.Excited/Excited_01.jpg  \n",
            "  inflating: botpeg/images/2.Excited/Excited_02.jpg  \n",
            "   creating: botpeg/images/3.Fight/\n",
            "  inflating: botpeg/images/3.Fight/Fight.jpg  \n",
            "   creating: botpeg/images/4.Funny/\n",
            "  inflating: botpeg/images/4.Funny/Funny_01.jpg  \n",
            "  inflating: botpeg/images/4.Funny/Funny_02.jpg  \n",
            "  inflating: botpeg/images/4.Funny/Funny_03.jpg  \n",
            "   creating: botpeg/images/5.Love/\n",
            "  inflating: botpeg/images/5.Love/Love.jpg  \n",
            "   creating: botpeg/images/6.Mad/\n",
            "  inflating: botpeg/images/6.Mad/Mad.jpg  \n",
            "   creating: botpeg/images/7.Sad/\n",
            "  inflating: botpeg/images/7.Sad/Sad.jpg  \n",
            "   creating: botpeg/images/8.Scared/\n",
            "  inflating: botpeg/images/8.Scared/Scared.jpg  \n",
            "   creating: botpeg/images/9.Shy/\n",
            "  inflating: botpeg/images/9.Shy/Shy.jpg  \n",
            "   creating: botpeg/parts/\n",
            "  inflating: botpeg/parts/part_click_locs.txt  \n",
            "  inflating: botpeg/parts/part_locs.txt  \n",
            "  inflating: botpeg/parts/parts.txt  \n",
            "  inflating: botpeg/train_test_split.txt  \n"
          ]
        }
      ],
      "source": [
        "!rm -rf botpeg # pegheads image-and-text-data\n",
        "!unzip botpeg.zip # pegheads.zip image-and-text-data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1b9ba0e532fe4a6892577c8d0126913f",
            "9920c754ca4540f7b1f15ba82d075aaa",
            "b72586872a494d2bb854c68c8a7f3e53",
            "5b67e3881873459cac132e46d20a4bdf",
            "ec7916cdc9ef4db59748cdafde9492ef",
            "59324b4498e544ad8c3686e2e6c9cabc",
            "d43c03c1d0754be1a1858eac01b9f18f",
            "4c927221d8e04023b7d47237cb69daa5"
          ]
        },
        "id": "Q3rPWNat1v3f",
        "outputId": "e6736800-6da3-4dd9-ea7d-079302e0171e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "63 images found for training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgavincapriola\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/gavincapriola/dalle_train_vae/runs/2ogtsahn\" target=\"_blank\">stilted-smoke-1</a></strong> to <a href=\"https://wandb.ai/gavincapriola/dalle_train_vae\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/utils.py:50: UserWarning: range will be deprecated, please use value_range instead.\n",
            "  warnings.warn(warning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 0 lr - 0.000980 loss - 0.30546608567237854\n",
            "0 10 lr - 0.000980 loss - 0.4175953269004822\n",
            "1 0 lr - 0.000960 loss - 0.19407251477241516\n",
            "1 10 lr - 0.000960 loss - 0.1429627239704132\n",
            "2 0 lr - 0.000941 loss - 0.15376992523670197\n",
            "2 10 lr - 0.000941 loss - 0.15679118037223816\n",
            "3 0 lr - 0.000922 loss - 0.15407182276248932\n",
            "3 10 lr - 0.000922 loss - 0.10308878868818283\n",
            "4 0 lr - 0.000904 loss - 0.10628040879964828\n",
            "4 10 lr - 0.000904 loss - 0.08862641453742981\n",
            "5 0 lr - 0.000886 loss - 0.08285088837146759\n",
            "5 10 lr - 0.000886 loss - 0.12013989686965942\n",
            "6 0 lr - 0.000868 loss - 0.09798455238342285\n",
            "6 10 lr - 0.000868 loss - 0.10497968643903732\n",
            "7 0 lr - 0.000851 loss - 0.0727556049823761\n",
            "7 10 lr - 0.000851 loss - 0.05901220440864563\n",
            "8 0 lr - 0.000834 loss - 0.055832818150520325\n",
            "8 10 lr - 0.000834 loss - 0.05361298844218254\n",
            "9 0 lr - 0.000817 loss - 0.05534543842077255\n",
            "9 10 lr - 0.000817 loss - 0.04819953441619873\n",
            "10 0 lr - 0.000801 loss - 0.04685979336500168\n",
            "10 10 lr - 0.000801 loss - 0.038423389196395874\n",
            "11 0 lr - 0.000785 loss - 0.05713069438934326\n",
            "11 10 lr - 0.000785 loss - 0.03420334309339523\n",
            "12 0 lr - 0.000769 loss - 0.032830432057380676\n",
            "12 10 lr - 0.000769 loss - 0.03454970568418503\n",
            "13 0 lr - 0.000754 loss - 0.02699589729309082\n",
            "13 10 lr - 0.000754 loss - 0.022635210305452347\n",
            "14 0 lr - 0.000739 loss - 0.02807467430830002\n",
            "14 10 lr - 0.000739 loss - 0.026395102962851524\n",
            "15 0 lr - 0.000724 loss - 0.024096770212054253\n",
            "15 10 lr - 0.000724 loss - 0.02860037051141262\n",
            "16 0 lr - 0.000709 loss - 0.027239223942160606\n",
            "16 10 lr - 0.000709 loss - 0.02672402560710907\n",
            "17 0 lr - 0.000695 loss - 0.02097242884337902\n",
            "17 10 lr - 0.000695 loss - 0.02827174961566925\n",
            "18 0 lr - 0.000681 loss - 0.025073327124118805\n",
            "18 10 lr - 0.000681 loss - 0.019777417182922363\n",
            "19 0 lr - 0.000668 loss - 0.016915960237383842\n",
            "19 10 lr - 0.000668 loss - 0.021926602348685265\n",
            "20 0 lr - 0.000654 loss - 0.024014905095100403\n",
            "20 10 lr - 0.000654 loss - 0.01669502817094326\n",
            "21 0 lr - 0.000641 loss - 0.02408633753657341\n",
            "21 10 lr - 0.000641 loss - 0.014803810976445675\n",
            "22 0 lr - 0.000628 loss - 0.018298599869012833\n",
            "22 10 lr - 0.000628 loss - 0.01759592816233635\n",
            "23 0 lr - 0.000616 loss - 0.022007446736097336\n",
            "23 10 lr - 0.000616 loss - 0.020741542801260948\n",
            "24 0 lr - 0.000603 loss - 0.014174573123455048\n",
            "24 10 lr - 0.000603 loss - 0.018610211089253426\n",
            "25 0 lr - 0.000591 loss - 0.020437411963939667\n",
            "25 10 lr - 0.000591 loss - 0.02073860913515091\n",
            "26 0 lr - 0.000580 loss - 0.019364170730113983\n",
            "26 10 lr - 0.000580 loss - 0.016284726560115814\n",
            "27 0 lr - 0.000568 loss - 0.019227609038352966\n",
            "27 10 lr - 0.000568 loss - 0.0181741900742054\n",
            "28 0 lr - 0.000557 loss - 0.01942017488181591\n",
            "28 10 lr - 0.000557 loss - 0.01784421131014824\n",
            "29 0 lr - 0.000545 loss - 0.014231523498892784\n",
            "29 10 lr - 0.000545 loss - 0.017285143956542015\n",
            "30 0 lr - 0.000535 loss - 0.01775681972503662\n",
            "30 10 lr - 0.000535 loss - 0.013575251214206219\n",
            "31 0 lr - 0.000524 loss - 0.019125601276755333\n",
            "31 10 lr - 0.000524 loss - 0.013395687565207481\n",
            "32 0 lr - 0.000513 loss - 0.018276097252964973\n",
            "32 10 lr - 0.000513 loss - 0.018501795828342438\n",
            "33 0 lr - 0.000503 loss - 0.017520375549793243\n",
            "33 10 lr - 0.000503 loss - 0.016628948971629143\n",
            "34 0 lr - 0.000493 loss - 0.0153772858902812\n",
            "34 10 lr - 0.000493 loss - 0.015472415834665298\n",
            "35 0 lr - 0.000483 loss - 0.016749337315559387\n",
            "35 10 lr - 0.000483 loss - 0.01388703566044569\n",
            "36 0 lr - 0.000474 loss - 0.01758457161486149\n",
            "36 10 lr - 0.000474 loss - 0.02063782513141632\n",
            "37 0 lr - 0.000464 loss - 0.0255323126912117\n",
            "37 10 lr - 0.000464 loss - 0.022532444447278976\n",
            "38 0 lr - 0.000455 loss - 0.01671997457742691\n",
            "38 10 lr - 0.000455 loss - 0.025867726653814316\n",
            "39 0 lr - 0.000446 loss - 0.020288541913032532\n",
            "39 10 lr - 0.000446 loss - 0.01645933836698532\n",
            "40 0 lr - 0.000437 loss - 0.01560275536030531\n",
            "40 10 lr - 0.000437 loss - 0.021352848038077354\n",
            "41 0 lr - 0.000428 loss - 0.016193924471735954\n",
            "41 10 lr - 0.000428 loss - 0.015958940610289574\n",
            "42 0 lr - 0.000419 loss - 0.017961610108613968\n",
            "42 10 lr - 0.000419 loss - 0.019024623557925224\n",
            "43 0 lr - 0.000411 loss - 0.017962325364351273\n",
            "43 10 lr - 0.000411 loss - 0.01627478376030922\n",
            "44 0 lr - 0.000403 loss - 0.015753302723169327\n",
            "44 10 lr - 0.000403 loss - 0.014521035365760326\n",
            "45 0 lr - 0.000395 loss - 0.014563491567969322\n",
            "45 10 lr - 0.000395 loss - 0.010849501006305218\n",
            "46 0 lr - 0.000387 loss - 0.014188434928655624\n",
            "46 10 lr - 0.000387 loss - 0.017031272873282433\n",
            "47 0 lr - 0.000379 loss - 0.011274978518486023\n",
            "47 10 lr - 0.000379 loss - 0.014770975336432457\n",
            "48 0 lr - 0.000372 loss - 0.014627615921199322\n",
            "48 10 lr - 0.000372 loss - 0.010907849296927452\n",
            "49 0 lr - 0.000364 loss - 0.015605843625962734\n",
            "49 10 lr - 0.000364 loss - 0.01513501163572073\n",
            "50 0 lr - 0.000357 loss - 0.011222517117857933\n",
            "50 10 lr - 0.000357 loss - 0.013484632596373558\n",
            "51 0 lr - 0.000350 loss - 0.008770530112087727\n",
            "51 10 lr - 0.000350 loss - 0.010832412168383598\n",
            "52 0 lr - 0.000343 loss - 0.009428861550986767\n",
            "52 10 lr - 0.000343 loss - 0.012532234191894531\n",
            "53 0 lr - 0.000336 loss - 0.008775503374636173\n",
            "53 10 lr - 0.000336 loss - 0.014527457766234875\n",
            "54 0 lr - 0.000329 loss - 0.009420434944331646\n",
            "54 10 lr - 0.000329 loss - 0.013211566023528576\n",
            "55 0 lr - 0.000323 loss - 0.01090993545949459\n",
            "55 10 lr - 0.000323 loss - 0.013737399131059647\n",
            "56 0 lr - 0.000316 loss - 0.013420658186078072\n",
            "56 10 lr - 0.000316 loss - 0.013206342235207558\n",
            "57 0 lr - 0.000310 loss - 0.011141501367092133\n",
            "57 10 lr - 0.000310 loss - 0.013946561142802238\n",
            "58 0 lr - 0.000304 loss - 0.011813279241323471\n",
            "58 10 lr - 0.000304 loss - 0.015956543385982513\n",
            "59 0 lr - 0.000298 loss - 0.01146756112575531\n",
            "59 10 lr - 0.000298 loss - 0.011685468256473541\n",
            "60 0 lr - 0.000292 loss - 0.010276280343532562\n",
            "60 10 lr - 0.000292 loss - 0.013207285664975643\n",
            "61 0 lr - 0.000286 loss - 0.014196749776601791\n",
            "61 10 lr - 0.000286 loss - 0.010866932570934296\n",
            "62 0 lr - 0.000280 loss - 0.010106068104505539\n",
            "62 10 lr - 0.000280 loss - 0.015246249735355377\n",
            "63 0 lr - 0.000274 loss - 0.012353687547147274\n",
            "63 10 lr - 0.000274 loss - 0.01103906612843275\n",
            "64 0 lr - 0.000269 loss - 0.012221412733197212\n",
            "64 10 lr - 0.000269 loss - 0.012152994982898235\n",
            "65 0 lr - 0.000264 loss - 0.010429661720991135\n",
            "65 10 lr - 0.000264 loss - 0.010487699881196022\n",
            "66 0 lr - 0.000258 loss - 0.01118442602455616\n",
            "66 10 lr - 0.000258 loss - 0.012906614691019058\n",
            "67 0 lr - 0.000253 loss - 0.013622939586639404\n",
            "67 10 lr - 0.000253 loss - 0.011480091139674187\n",
            "68 0 lr - 0.000248 loss - 0.012755915522575378\n",
            "68 10 lr - 0.000248 loss - 0.010906532406806946\n",
            "69 0 lr - 0.000243 loss - 0.01182953268289566\n",
            "69 10 lr - 0.000243 loss - 0.010399136692285538\n",
            "70 0 lr - 0.000238 loss - 0.010120367631316185\n",
            "70 10 lr - 0.000238 loss - 0.008619852364063263\n",
            "71 0 lr - 0.000233 loss - 0.011409255675971508\n",
            "71 10 lr - 0.000233 loss - 0.010136629454791546\n",
            "72 0 lr - 0.000229 loss - 0.009289886802434921\n",
            "72 10 lr - 0.000229 loss - 0.009265776723623276\n",
            "73 0 lr - 0.000224 loss - 0.011027595028281212\n",
            "73 10 lr - 0.000224 loss - 0.010487276129424572\n",
            "74 0 lr - 0.000220 loss - 0.007519350852817297\n",
            "74 10 lr - 0.000220 loss - 0.012392444536089897\n",
            "75 0 lr - 0.000215 loss - 0.011982987634837627\n",
            "75 10 lr - 0.000215 loss - 0.009851396083831787\n",
            "76 0 lr - 0.000211 loss - 0.009330540895462036\n",
            "76 10 lr - 0.000211 loss - 0.011849554255604744\n",
            "77 0 lr - 0.000207 loss - 0.011023811995983124\n",
            "77 10 lr - 0.000207 loss - 0.011136713437736034\n",
            "78 0 lr - 0.000203 loss - 0.010084292851388454\n",
            "78 10 lr - 0.000203 loss - 0.010633647441864014\n",
            "79 0 lr - 0.000199 loss - 0.008744336664676666\n",
            "79 10 lr - 0.000199 loss - 0.011956426315009594\n",
            "80 0 lr - 0.000195 loss - 0.009517822414636612\n",
            "80 10 lr - 0.000195 loss - 0.0102546326816082\n",
            "81 0 lr - 0.000191 loss - 0.01012422889471054\n",
            "81 10 lr - 0.000191 loss - 0.010826200246810913\n",
            "82 0 lr - 0.000187 loss - 0.01161182951182127\n",
            "82 10 lr - 0.000187 loss - 0.0113265011459589\n",
            "83 0 lr - 0.000183 loss - 0.01055426150560379\n",
            "83 10 lr - 0.000183 loss - 0.010487334802746773\n",
            "84 0 lr - 0.000180 loss - 0.011457658372819424\n",
            "84 10 lr - 0.000180 loss - 0.01067272201180458\n",
            "85 0 lr - 0.000176 loss - 0.008875599130988121\n",
            "85 10 lr - 0.000176 loss - 0.012359080836176872\n",
            "86 0 lr - 0.000172 loss - 0.009183557704091072\n",
            "86 10 lr - 0.000172 loss - 0.011421028524637222\n",
            "87 0 lr - 0.000169 loss - 0.011723622679710388\n",
            "87 10 lr - 0.000169 loss - 0.009350668638944626\n",
            "88 0 lr - 0.000166 loss - 0.009891342371702194\n",
            "88 10 lr - 0.000166 loss - 0.009677482768893242\n",
            "89 0 lr - 0.000162 loss - 0.008267886936664581\n",
            "89 10 lr - 0.000162 loss - 0.008853345178067684\n",
            "90 0 lr - 0.000159 loss - 0.005801384802907705\n",
            "90 10 lr - 0.000159 loss - 0.007958867587149143\n",
            "91 0 lr - 0.000156 loss - 0.010452482849359512\n",
            "91 10 lr - 0.000156 loss - 0.01025203987956047\n",
            "92 0 lr - 0.000153 loss - 0.010256532579660416\n",
            "92 10 lr - 0.000153 loss - 0.008735421113669872\n",
            "93 0 lr - 0.000150 loss - 0.012759624980390072\n",
            "93 10 lr - 0.000150 loss - 0.008128894492983818\n",
            "94 0 lr - 0.000147 loss - 0.009455407038331032\n",
            "94 10 lr - 0.000147 loss - 0.011060445569455624\n",
            "95 0 lr - 0.000144 loss - 0.00946863368153572\n",
            "95 10 lr - 0.000144 loss - 0.010202189907431602\n",
            "96 0 lr - 0.000141 loss - 0.0103164566680789\n",
            "96 10 lr - 0.000141 loss - 0.009811392053961754\n",
            "97 0 lr - 0.000138 loss - 0.01019775029271841\n",
            "97 10 lr - 0.000138 loss - 0.009189825505018234\n",
            "98 0 lr - 0.000135 loss - 0.00837203860282898\n",
            "98 10 lr - 0.000135 loss - 0.009663643315434456\n",
            "99 0 lr - 0.000133 loss - 0.011331034824252129\n",
            "99 10 lr - 0.000133 loss - 0.009698295965790749\n",
            "100 0 lr - 0.000130 loss - 0.008396787568926811\n",
            "100 10 lr - 0.000130 loss - 0.012169587425887585\n",
            "101 0 lr - 0.000127 loss - 0.009106317535042763\n",
            "101 10 lr - 0.000127 loss - 0.011058388277888298\n",
            "102 0 lr - 0.000125 loss - 0.009132984094321728\n",
            "102 10 lr - 0.000125 loss - 0.009382561780512333\n",
            "103 0 lr - 0.000122 loss - 0.009719893336296082\n",
            "103 10 lr - 0.000122 loss - 0.008420093916356564\n",
            "104 0 lr - 0.000120 loss - 0.009701592847704887\n",
            "104 10 lr - 0.000120 loss - 0.005794649478048086\n",
            "105 0 lr - 0.000117 loss - 0.009772242978215218\n",
            "105 10 lr - 0.000117 loss - 0.009367434307932854\n",
            "106 0 lr - 0.000115 loss - 0.010790308937430382\n",
            "106 10 lr - 0.000115 loss - 0.007624272257089615\n",
            "107 0 lr - 0.000113 loss - 0.009261966682970524\n",
            "107 10 lr - 0.000113 loss - 0.009368108585476875\n",
            "108 0 lr - 0.000111 loss - 0.009706895798444748\n",
            "108 10 lr - 0.000111 loss - 0.008167016319930553\n",
            "109 0 lr - 0.000108 loss - 0.010229199193418026\n",
            "109 10 lr - 0.000108 loss - 0.010551432147622108\n",
            "110 0 lr - 0.000106 loss - 0.006641380488872528\n",
            "110 10 lr - 0.000106 loss - 0.009540960192680359\n",
            "111 0 lr - 0.000104 loss - 0.007355936802923679\n",
            "111 10 lr - 0.000104 loss - 0.007754945196211338\n",
            "112 0 lr - 0.000102 loss - 0.007779492996633053\n",
            "112 10 lr - 0.000102 loss - 0.008423189632594585\n",
            "113 0 lr - 0.000100 loss - 0.007094839587807655\n",
            "113 10 lr - 0.000100 loss - 0.0075021833181381226\n",
            "114 0 lr - 0.000098 loss - 0.010443449020385742\n",
            "114 10 lr - 0.000098 loss - 0.0072060804814100266\n",
            "115 0 lr - 0.000096 loss - 0.009624435566365719\n",
            "115 10 lr - 0.000096 loss - 0.008690893650054932\n",
            "116 0 lr - 0.000094 loss - 0.009728601202368736\n",
            "116 10 lr - 0.000094 loss - 0.009458204731345177\n",
            "117 0 lr - 0.000092 loss - 0.0082797110080719\n",
            "117 10 lr - 0.000092 loss - 0.007411378435790539\n",
            "118 0 lr - 0.000090 loss - 0.008009707555174828\n",
            "118 10 lr - 0.000090 loss - 0.0068735661916434765\n",
            "119 0 lr - 0.000089 loss - 0.007920547388494015\n",
            "119 10 lr - 0.000089 loss - 0.007370931562036276\n",
            "120 0 lr - 0.000087 loss - 0.010663703083992004\n",
            "120 10 lr - 0.000087 loss - 0.008865665644407272\n",
            "121 0 lr - 0.000085 loss - 0.00874230358749628\n",
            "121 10 lr - 0.000085 loss - 0.011783103458583355\n",
            "122 0 lr - 0.000083 loss - 0.0076461052522063255\n",
            "122 10 lr - 0.000083 loss - 0.00869044940918684\n",
            "123 0 lr - 0.000082 loss - 0.008981352671980858\n",
            "123 10 lr - 0.000082 loss - 0.010093667544424534\n",
            "124 0 lr - 0.000080 loss - 0.007874231785535812\n",
            "124 10 lr - 0.000080 loss - 0.007054880261421204\n",
            "125 0 lr - 0.000078 loss - 0.008356300182640553\n",
            "125 10 lr - 0.000078 loss - 0.008065082132816315\n",
            "126 0 lr - 0.000077 loss - 0.009827256202697754\n",
            "126 10 lr - 0.000077 loss - 0.006088656838983297\n",
            "127 0 lr - 0.000075 loss - 0.009655851870775223\n",
            "127 10 lr - 0.000075 loss - 0.006998020689934492\n",
            "128 0 lr - 0.000074 loss - 0.007891153916716576\n",
            "128 10 lr - 0.000074 loss - 0.00868711993098259\n",
            "129 0 lr - 0.000072 loss - 0.008165225386619568\n",
            "129 10 lr - 0.000072 loss - 0.009236029349267483\n",
            "130 0 lr - 0.000071 loss - 0.00833061896264553\n",
            "130 10 lr - 0.000071 loss - 0.00808392371982336\n",
            "131 0 lr - 0.000069 loss - 0.009176168590784073\n",
            "131 10 lr - 0.000069 loss - 0.006278466433286667\n",
            "132 0 lr - 0.000068 loss - 0.005947036202996969\n",
            "132 10 lr - 0.000068 loss - 0.007449520751833916\n",
            "133 0 lr - 0.000067 loss - 0.006075291894376278\n",
            "133 10 lr - 0.000067 loss - 0.006148622836917639\n",
            "134 0 lr - 0.000065 loss - 0.00630501564592123\n",
            "134 10 lr - 0.000065 loss - 0.009340357035398483\n",
            "135 0 lr - 0.000064 loss - 0.008698086254298687\n",
            "135 10 lr - 0.000064 loss - 0.009065534919500351\n",
            "136 0 lr - 0.000063 loss - 0.008711004629731178\n",
            "136 10 lr - 0.000063 loss - 0.006110440939664841\n",
            "137 0 lr - 0.000062 loss - 0.011146820150315762\n",
            "137 10 lr - 0.000062 loss - 0.009808667004108429\n",
            "138 0 lr - 0.000060 loss - 0.009479071013629436\n",
            "138 10 lr - 0.000060 loss - 0.006734819617122412\n",
            "139 0 lr - 0.000059 loss - 0.007363173179328442\n",
            "139 10 lr - 0.000059 loss - 0.009878421202301979\n",
            "140 0 lr - 0.000058 loss - 0.00835149735212326\n",
            "140 10 lr - 0.000058 loss - 0.006918319966644049\n",
            "141 0 lr - 0.000057 loss - 0.008309215307235718\n",
            "141 10 lr - 0.000057 loss - 0.009283876977860928\n",
            "142 0 lr - 0.000056 loss - 0.00825786404311657\n",
            "142 10 lr - 0.000056 loss - 0.007858051918447018\n",
            "143 0 lr - 0.000055 loss - 0.007224328350275755\n",
            "143 10 lr - 0.000055 loss - 0.0072271875105798244\n",
            "144 0 lr - 0.000053 loss - 0.008006155490875244\n",
            "144 10 lr - 0.000053 loss - 0.0065832240507006645\n",
            "145 0 lr - 0.000052 loss - 0.008510582149028778\n",
            "145 10 lr - 0.000052 loss - 0.007666582241654396\n",
            "146 0 lr - 0.000051 loss - 0.006250046193599701\n",
            "146 10 lr - 0.000051 loss - 0.009120006114244461\n",
            "147 0 lr - 0.000050 loss - 0.005514098331332207\n",
            "147 10 lr - 0.000050 loss - 0.0076873949728906155\n",
            "148 0 lr - 0.000049 loss - 0.007539615035057068\n",
            "148 10 lr - 0.000049 loss - 0.009775918908417225\n",
            "149 0 lr - 0.000048 loss - 0.007249883376061916\n",
            "149 10 lr - 0.000048 loss - 0.008337254635989666\n",
            "150 0 lr - 0.000047 loss - 0.008039523847401142\n",
            "150 10 lr - 0.000047 loss - 0.00947875902056694\n",
            "151 0 lr - 0.000046 loss - 0.00870816595852375\n",
            "151 10 lr - 0.000046 loss - 0.008666319772601128\n",
            "152 0 lr - 0.000045 loss - 0.007482850458472967\n",
            "152 10 lr - 0.000045 loss - 0.00822746753692627\n",
            "153 0 lr - 0.000045 loss - 0.008703047409653664\n",
            "153 10 lr - 0.000045 loss - 0.0065526701509952545\n",
            "154 0 lr - 0.000044 loss - 0.005467705428600311\n",
            "154 10 lr - 0.000044 loss - 0.008398065343499184\n",
            "155 0 lr - 0.000043 loss - 0.006884129252284765\n",
            "155 10 lr - 0.000043 loss - 0.009192335419356823\n",
            "156 0 lr - 0.000042 loss - 0.009162042289972305\n",
            "156 10 lr - 0.000042 loss - 0.008543678559362888\n",
            "157 0 lr - 0.000041 loss - 0.007685042917728424\n",
            "157 10 lr - 0.000041 loss - 0.008257776498794556\n",
            "158 0 lr - 0.000040 loss - 0.009579960256814957\n",
            "158 10 lr - 0.000040 loss - 0.007672255393117666\n",
            "159 0 lr - 0.000039 loss - 0.00938219390809536\n",
            "159 10 lr - 0.000039 loss - 0.007343007251620293\n",
            "160 0 lr - 0.000039 loss - 0.008008974604308605\n",
            "160 10 lr - 0.000039 loss - 0.006417608819901943\n",
            "161 0 lr - 0.000038 loss - 0.0054221488535404205\n",
            "161 10 lr - 0.000038 loss - 0.006880742963403463\n",
            "162 0 lr - 0.000037 loss - 0.011047346517443657\n",
            "162 10 lr - 0.000037 loss - 0.009468890726566315\n",
            "163 0 lr - 0.000036 loss - 0.007094858214259148\n",
            "163 10 lr - 0.000036 loss - 0.006159828510135412\n",
            "164 0 lr - 0.000036 loss - 0.009216271340847015\n",
            "164 10 lr - 0.000036 loss - 0.008704960346221924\n",
            "165 0 lr - 0.000035 loss - 0.00873081386089325\n",
            "165 10 lr - 0.000035 loss - 0.008888117037713528\n",
            "166 0 lr - 0.000034 loss - 0.008676229976117611\n",
            "166 10 lr - 0.000034 loss - 0.007110752165317535\n",
            "167 0 lr - 0.000034 loss - 0.00662868469953537\n",
            "167 10 lr - 0.000034 loss - 0.0055012088268995285\n",
            "168 0 lr - 0.000033 loss - 0.006211509928107262\n",
            "168 10 lr - 0.000033 loss - 0.0058161295019090176\n",
            "169 0 lr - 0.000032 loss - 0.0071668969467282295\n",
            "169 10 lr - 0.000032 loss - 0.006170231848955154\n",
            "170 0 lr - 0.000032 loss - 0.007552034221589565\n",
            "170 10 lr - 0.000032 loss - 0.00796870794147253\n",
            "171 0 lr - 0.000031 loss - 0.007586547173559666\n",
            "171 10 lr - 0.000031 loss - 0.005778113380074501\n",
            "172 0 lr - 0.000030 loss - 0.007459738757461309\n",
            "172 10 lr - 0.000030 loss - 0.006904733832925558\n",
            "173 0 lr - 0.000030 loss - 0.008932742290198803\n",
            "173 10 lr - 0.000030 loss - 0.010214181616902351\n",
            "174 0 lr - 0.000029 loss - 0.008085932582616806\n",
            "174 10 lr - 0.000029 loss - 0.006755998358130455\n",
            "175 0 lr - 0.000029 loss - 0.007974961772561073\n",
            "175 10 lr - 0.000029 loss - 0.00761780422180891\n",
            "176 0 lr - 0.000028 loss - 0.00674107950180769\n",
            "176 10 lr - 0.000028 loss - 0.006236395798623562\n",
            "177 0 lr - 0.000027 loss - 0.007054632529616356\n",
            "177 10 lr - 0.000027 loss - 0.005764500238001347\n",
            "178 0 lr - 0.000027 loss - 0.006223483011126518\n",
            "178 10 lr - 0.000027 loss - 0.008508092723786831\n",
            "179 0 lr - 0.000026 loss - 0.006214491557329893\n",
            "179 10 lr - 0.000026 loss - 0.006600047927349806\n",
            "180 0 lr - 0.000026 loss - 0.007786495145410299\n",
            "180 10 lr - 0.000026 loss - 0.007648786064237356\n",
            "181 0 lr - 0.000025 loss - 0.007798345293849707\n",
            "181 10 lr - 0.000025 loss - 0.005732973106205463\n",
            "182 0 lr - 0.000025 loss - 0.0064889597706496716\n",
            "182 10 lr - 0.000025 loss - 0.007074691355228424\n",
            "183 0 lr - 0.000024 loss - 0.006552164442837238\n",
            "183 10 lr - 0.000024 loss - 0.009770798496901989\n",
            "184 0 lr - 0.000024 loss - 0.008434602990746498\n",
            "184 10 lr - 0.000024 loss - 0.008249007165431976\n",
            "185 0 lr - 0.000023 loss - 0.00608267355710268\n",
            "185 10 lr - 0.000023 loss - 0.008338578045368195\n",
            "186 0 lr - 0.000023 loss - 0.006868693511933088\n",
            "186 10 lr - 0.000023 loss - 0.006946136709302664\n",
            "187 0 lr - 0.000022 loss - 0.007385928183794022\n",
            "187 10 lr - 0.000022 loss - 0.007350347936153412\n",
            "188 0 lr - 0.000022 loss - 0.008480214513838291\n",
            "188 10 lr - 0.000022 loss - 0.007414649240672588\n",
            "189 0 lr - 0.000022 loss - 0.008800795301795006\n",
            "189 10 lr - 0.000022 loss - 0.0071571990847587585\n",
            "190 0 lr - 0.000021 loss - 0.008752983063459396\n",
            "190 10 lr - 0.000021 loss - 0.006299001630395651\n",
            "191 0 lr - 0.000021 loss - 0.0064343176782131195\n",
            "191 10 lr - 0.000021 loss - 0.006720702163875103\n",
            "192 0 lr - 0.000020 loss - 0.007571512833237648\n",
            "192 10 lr - 0.000020 loss - 0.009324999526143074\n",
            "193 0 lr - 0.000020 loss - 0.008050870150327682\n",
            "193 10 lr - 0.000020 loss - 0.01009108405560255\n",
            "194 0 lr - 0.000019 loss - 0.006120671983808279\n",
            "194 10 lr - 0.000019 loss - 0.009047074243426323\n",
            "195 0 lr - 0.000019 loss - 0.007352493703365326\n",
            "195 10 lr - 0.000019 loss - 0.009287113323807716\n",
            "196 0 lr - 0.000019 loss - 0.005796859972178936\n",
            "196 10 lr - 0.000019 loss - 0.005608642008155584\n",
            "197 0 lr - 0.000018 loss - 0.0073981205932796\n",
            "197 10 lr - 0.000018 loss - 0.006268096622079611\n",
            "198 0 lr - 0.000018 loss - 0.007667961996048689\n",
            "198 10 lr - 0.000018 loss - 0.008590798825025558\n",
            "199 0 lr - 0.000018 loss - 0.005967144854366779\n",
            "199 10 lr - 0.000018 loss - 0.009081631898880005\n",
            "200 0 lr - 0.000017 loss - 0.007160873152315617\n",
            "200 10 lr - 0.000017 loss - 0.00781534519046545\n",
            "201 0 lr - 0.000017 loss - 0.007500100880861282\n",
            "201 10 lr - 0.000017 loss - 0.007664657663553953\n",
            "202 0 lr - 0.000017 loss - 0.006644511129707098\n",
            "202 10 lr - 0.000017 loss - 0.006294421851634979\n",
            "203 0 lr - 0.000016 loss - 0.008140664547681808\n",
            "203 10 lr - 0.000016 loss - 0.008040124550461769\n",
            "204 0 lr - 0.000016 loss - 0.005200814921408892\n",
            "204 10 lr - 0.000016 loss - 0.006312453653663397\n",
            "205 0 lr - 0.000016 loss - 0.00834936648607254\n",
            "205 10 lr - 0.000016 loss - 0.007287533953785896\n",
            "206 0 lr - 0.000015 loss - 0.008670056238770485\n",
            "206 10 lr - 0.000015 loss - 0.009090336039662361\n",
            "207 0 lr - 0.000015 loss - 0.009182906709611416\n",
            "207 10 lr - 0.000015 loss - 0.006105941720306873\n",
            "208 0 lr - 0.000015 loss - 0.007861431688070297\n",
            "208 10 lr - 0.000015 loss - 0.007207551505416632\n",
            "209 0 lr - 0.000014 loss - 0.006990628316998482\n",
            "209 10 lr - 0.000014 loss - 0.007570920046418905\n",
            "210 0 lr - 0.000014 loss - 0.007213521748781204\n",
            "210 10 lr - 0.000014 loss - 0.00881771370768547\n",
            "211 0 lr - 0.000014 loss - 0.006846732459962368\n",
            "211 10 lr - 0.000014 loss - 0.009888801723718643\n",
            "212 0 lr - 0.000014 loss - 0.008479123003780842\n",
            "212 10 lr - 0.000014 loss - 0.006761375814676285\n",
            "213 0 lr - 0.000013 loss - 0.006576017942279577\n",
            "213 10 lr - 0.000013 loss - 0.007711105048656464\n",
            "214 0 lr - 0.000013 loss - 0.006821056827902794\n",
            "214 10 lr - 0.000013 loss - 0.008549879305064678\n",
            "215 0 lr - 0.000013 loss - 0.008097306825220585\n",
            "215 10 lr - 0.000013 loss - 0.005266065709292889\n",
            "216 0 lr - 0.000012 loss - 0.00697894673794508\n",
            "216 10 lr - 0.000012 loss - 0.007352438755333424\n",
            "217 0 lr - 0.000012 loss - 0.004666398279368877\n",
            "217 10 lr - 0.000012 loss - 0.007939628325402737\n",
            "218 0 lr - 0.000012 loss - 0.007793141063302755\n",
            "218 10 lr - 0.000012 loss - 0.006317831575870514\n",
            "219 0 lr - 0.000012 loss - 0.006601050496101379\n",
            "219 10 lr - 0.000012 loss - 0.007194322533905506\n",
            "220 0 lr - 0.000012 loss - 0.00755778094753623\n",
            "220 10 lr - 0.000012 loss - 0.0052442774176597595\n",
            "221 0 lr - 0.000011 loss - 0.005528686102479696\n",
            "221 10 lr - 0.000011 loss - 0.009862856939435005\n",
            "222 0 lr - 0.000011 loss - 0.008827010169625282\n",
            "222 10 lr - 0.000011 loss - 0.005846712738275528\n",
            "223 0 lr - 0.000011 loss - 0.00888940878212452\n",
            "223 10 lr - 0.000011 loss - 0.006962876301258802\n",
            "224 0 lr - 0.000011 loss - 0.007595764007419348\n",
            "224 10 lr - 0.000011 loss - 0.006741940975189209\n",
            "225 0 lr - 0.000010 loss - 0.006905247922986746\n",
            "225 10 lr - 0.000010 loss - 0.006756865419447422\n",
            "226 0 lr - 0.000010 loss - 0.005666270852088928\n",
            "226 10 lr - 0.000010 loss - 0.007961357943713665\n",
            "227 0 lr - 0.000010 loss - 0.00741136260330677\n",
            "227 10 lr - 0.000010 loss - 0.008603282272815704\n",
            "228 0 lr - 0.000010 loss - 0.006785753648728132\n",
            "228 10 lr - 0.000010 loss - 0.006919502280652523\n",
            "229 0 lr - 0.000010 loss - 0.008804388344287872\n",
            "229 10 lr - 0.000010 loss - 0.009941764175891876\n",
            "230 0 lr - 0.000009 loss - 0.00931557547301054\n",
            "230 10 lr - 0.000009 loss - 0.008138084784150124\n",
            "231 0 lr - 0.000009 loss - 0.006733576767146587\n",
            "231 10 lr - 0.000009 loss - 0.0074432059191167355\n",
            "232 0 lr - 0.000009 loss - 0.007373685948550701\n",
            "232 10 lr - 0.000009 loss - 0.007332383655011654\n",
            "233 0 lr - 0.000009 loss - 0.0072377934120595455\n",
            "233 10 lr - 0.000009 loss - 0.005364683922380209\n",
            "234 0 lr - 0.000009 loss - 0.00597612839192152\n",
            "234 10 lr - 0.000009 loss - 0.007204489316791296\n",
            "235 0 lr - 0.000008 loss - 0.007622962351888418\n",
            "235 10 lr - 0.000008 loss - 0.006434640847146511\n",
            "236 0 lr - 0.000008 loss - 0.010188299231231213\n",
            "236 10 lr - 0.000008 loss - 0.006557719316333532\n",
            "237 0 lr - 0.000008 loss - 0.006745549850165844\n",
            "237 10 lr - 0.000008 loss - 0.007887598127126694\n",
            "238 0 lr - 0.000008 loss - 0.005743063986301422\n",
            "238 10 lr - 0.000008 loss - 0.008175654336810112\n",
            "239 0 lr - 0.000008 loss - 0.00841327290982008\n",
            "239 10 lr - 0.000008 loss - 0.006077158730477095\n",
            "240 0 lr - 0.000008 loss - 0.005710627883672714\n",
            "240 10 lr - 0.000008 loss - 0.006966705434024334\n",
            "241 0 lr - 0.000008 loss - 0.006665008142590523\n",
            "241 10 lr - 0.000008 loss - 0.007487724535167217\n",
            "242 0 lr - 0.000007 loss - 0.006974501069635153\n",
            "242 10 lr - 0.000007 loss - 0.008222142234444618\n",
            "243 0 lr - 0.000007 loss - 0.008430041372776031\n",
            "243 10 lr - 0.000007 loss - 0.007241418119519949\n",
            "244 0 lr - 0.000007 loss - 0.006507806479930878\n",
            "244 10 lr - 0.000007 loss - 0.006533758714795113\n",
            "245 0 lr - 0.000007 loss - 0.008989989757537842\n",
            "245 10 lr - 0.000007 loss - 0.006909426301717758\n",
            "246 0 lr - 0.000007 loss - 0.007474084850400686\n",
            "246 10 lr - 0.000007 loss - 0.008146263659000397\n",
            "247 0 lr - 0.000007 loss - 0.00629068911075592\n",
            "247 10 lr - 0.000007 loss - 0.00762533163651824\n",
            "248 0 lr - 0.000007 loss - 0.008214516565203667\n",
            "248 10 lr - 0.000007 loss - 0.0071682739071547985\n",
            "249 0 lr - 0.000006 loss - 0.006796424742788076\n",
            "249 10 lr - 0.000006 loss - 0.00754521694034338\n",
            "250 0 lr - 0.000006 loss - 0.00655417051166296\n",
            "250 10 lr - 0.000006 loss - 0.008071474730968475\n",
            "251 0 lr - 0.000006 loss - 0.0064575462602078915\n",
            "251 10 lr - 0.000006 loss - 0.007448028307408094\n",
            "252 0 lr - 0.000006 loss - 0.007525527849793434\n",
            "252 10 lr - 0.000006 loss - 0.007467485032975674\n",
            "253 0 lr - 0.000006 loss - 0.0070373062044382095\n",
            "253 10 lr - 0.000006 loss - 0.007913603447377682\n",
            "254 0 lr - 0.000006 loss - 0.008134918287396431\n",
            "254 10 lr - 0.000006 loss - 0.00602541770786047\n",
            "255 0 lr - 0.000006 loss - 0.008515240624547005\n",
            "255 10 lr - 0.000006 loss - 0.007360401563346386\n",
            "256 0 lr - 0.000006 loss - 0.006598161067813635\n",
            "256 10 lr - 0.000006 loss - 0.00611130241304636\n",
            "257 0 lr - 0.000005 loss - 0.00539217097684741\n",
            "257 10 lr - 0.000005 loss - 0.006446155719459057\n",
            "258 0 lr - 0.000005 loss - 0.007506050169467926\n",
            "258 10 lr - 0.000005 loss - 0.008339829742908478\n",
            "259 0 lr - 0.000005 loss - 0.007742624264210463\n",
            "259 10 lr - 0.000005 loss - 0.008154550567269325\n",
            "260 0 lr - 0.000005 loss - 0.006747470237314701\n",
            "260 10 lr - 0.000005 loss - 0.007406905293464661\n",
            "261 0 lr - 0.000005 loss - 0.006741621531546116\n",
            "261 10 lr - 0.000005 loss - 0.006297861225903034\n",
            "262 0 lr - 0.000005 loss - 0.007260136306285858\n",
            "262 10 lr - 0.000005 loss - 0.008351962082087994\n",
            "263 0 lr - 0.000005 loss - 0.006516215391457081\n",
            "263 10 lr - 0.000005 loss - 0.007023022510111332\n",
            "264 0 lr - 0.000005 loss - 0.007007911801338196\n",
            "264 10 lr - 0.000005 loss - 0.005000297445803881\n",
            "265 0 lr - 0.000005 loss - 0.00914042629301548\n",
            "265 10 lr - 0.000005 loss - 0.006967227440327406\n",
            "266 0 lr - 0.000005 loss - 0.005645833909511566\n",
            "266 10 lr - 0.000005 loss - 0.00696833711117506\n",
            "267 0 lr - 0.000004 loss - 0.006285542156547308\n",
            "267 10 lr - 0.000004 loss - 0.007596650626510382\n",
            "268 0 lr - 0.000004 loss - 0.006551722995936871\n",
            "268 10 lr - 0.000004 loss - 0.007809620350599289\n",
            "269 0 lr - 0.000004 loss - 0.006781560834497213\n",
            "269 10 lr - 0.000004 loss - 0.008849089965224266\n",
            "270 0 lr - 0.000004 loss - 0.00784386321902275\n",
            "270 10 lr - 0.000004 loss - 0.007294340059161186\n",
            "271 0 lr - 0.000004 loss - 0.007439160253852606\n",
            "271 10 lr - 0.000004 loss - 0.006960711441934109\n",
            "272 0 lr - 0.000004 loss - 0.0072275688871741295\n",
            "272 10 lr - 0.000004 loss - 0.006515956483781338\n",
            "273 0 lr - 0.000004 loss - 0.00930437259376049\n",
            "273 10 lr - 0.000004 loss - 0.008529113605618477\n",
            "274 0 lr - 0.000004 loss - 0.005661483854055405\n",
            "274 10 lr - 0.000004 loss - 0.007796274032443762\n",
            "275 0 lr - 0.000004 loss - 0.006955806165933609\n",
            "275 10 lr - 0.000004 loss - 0.008128403685986996\n",
            "276 0 lr - 0.000004 loss - 0.008216451853513718\n",
            "276 10 lr - 0.000004 loss - 0.005451067816466093\n",
            "277 0 lr - 0.000004 loss - 0.009716419503092766\n",
            "277 10 lr - 0.000004 loss - 0.005629085935652256\n",
            "278 0 lr - 0.000004 loss - 0.0062405108474195\n",
            "278 10 lr - 0.000004 loss - 0.00540126021951437\n",
            "279 0 lr - 0.000003 loss - 0.007439113222062588\n",
            "279 10 lr - 0.000003 loss - 0.005977470427751541\n",
            "280 0 lr - 0.000003 loss - 0.008439029566943645\n",
            "280 10 lr - 0.000003 loss - 0.007957151159644127\n",
            "281 0 lr - 0.000003 loss - 0.007222806569188833\n",
            "281 10 lr - 0.000003 loss - 0.0081716850399971\n",
            "282 0 lr - 0.000003 loss - 0.005035193637013435\n",
            "282 10 lr - 0.000003 loss - 0.00833978969603777\n",
            "283 0 lr - 0.000003 loss - 0.006238594185560942\n",
            "283 10 lr - 0.000003 loss - 0.008140049874782562\n",
            "284 0 lr - 0.000003 loss - 0.008142847567796707\n",
            "284 10 lr - 0.000003 loss - 0.008174370042979717\n",
            "285 0 lr - 0.000003 loss - 0.008498923853039742\n",
            "285 10 lr - 0.000003 loss - 0.007379799149930477\n",
            "286 0 lr - 0.000003 loss - 0.008472288027405739\n",
            "286 10 lr - 0.000003 loss - 0.008980674669146538\n",
            "287 0 lr - 0.000003 loss - 0.009276486933231354\n",
            "287 10 lr - 0.000003 loss - 0.007449820637702942\n",
            "288 0 lr - 0.000003 loss - 0.006985228508710861\n",
            "288 10 lr - 0.000003 loss - 0.0057057468220591545\n",
            "289 0 lr - 0.000003 loss - 0.00608088169246912\n",
            "289 10 lr - 0.000003 loss - 0.00789373554289341\n",
            "290 0 lr - 0.000003 loss - 0.007855123840272427\n",
            "290 10 lr - 0.000003 loss - 0.006294271908700466\n",
            "291 0 lr - 0.000003 loss - 0.008517410606145859\n",
            "291 10 lr - 0.000003 loss - 0.0068573555909097195\n",
            "292 0 lr - 0.000003 loss - 0.006773877888917923\n",
            "292 10 lr - 0.000003 loss - 0.005747203715145588\n",
            "293 0 lr - 0.000003 loss - 0.007653904613107443\n",
            "293 10 lr - 0.000003 loss - 0.007607563398778439\n",
            "294 0 lr - 0.000003 loss - 0.007534237578511238\n",
            "294 10 lr - 0.000003 loss - 0.008383062668144703\n",
            "295 0 lr - 0.000003 loss - 0.008869447745382786\n",
            "295 10 lr - 0.000003 loss - 0.00910467654466629\n",
            "296 0 lr - 0.000002 loss - 0.006699958350509405\n",
            "296 10 lr - 0.000002 loss - 0.006627349182963371\n",
            "297 0 lr - 0.000002 loss - 0.007480195723474026\n",
            "297 10 lr - 0.000002 loss - 0.007466153707355261\n",
            "298 0 lr - 0.000002 loss - 0.007218369748443365\n",
            "298 10 lr - 0.000002 loss - 0.007879847660660744\n",
            "299 0 lr - 0.000002 loss - 0.00715960469096899\n",
            "299 10 lr - 0.000002 loss - 0.007025295402854681\n",
            "300 0 lr - 0.000002 loss - 0.004829852841794491\n",
            "300 10 lr - 0.000002 loss - 0.004895078483968973\n",
            "301 0 lr - 0.000002 loss - 0.006620825733989477\n",
            "301 10 lr - 0.000002 loss - 0.006348919123411179\n",
            "302 0 lr - 0.000002 loss - 0.00764300674200058\n",
            "302 10 lr - 0.000002 loss - 0.00459706038236618\n",
            "303 0 lr - 0.000002 loss - 0.0045317793264985085\n",
            "303 10 lr - 0.000002 loss - 0.009221979416906834\n",
            "304 0 lr - 0.000002 loss - 0.006217196583747864\n",
            "304 10 lr - 0.000002 loss - 0.009064750745892525\n",
            "305 0 lr - 0.000002 loss - 0.008827905170619488\n",
            "305 10 lr - 0.000002 loss - 0.004407558590173721\n",
            "306 0 lr - 0.000002 loss - 0.00949360802769661\n",
            "306 10 lr - 0.000002 loss - 0.006129439454525709\n",
            "307 0 lr - 0.000002 loss - 0.007515258155763149\n",
            "307 10 lr - 0.000002 loss - 0.008637884631752968\n",
            "308 0 lr - 0.000002 loss - 0.00729776918888092\n",
            "308 10 lr - 0.000002 loss - 0.006239631678909063\n",
            "309 0 lr - 0.000002 loss - 0.008312744088470936\n",
            "309 10 lr - 0.000002 loss - 0.00735608022660017\n",
            "310 0 lr - 0.000002 loss - 0.007794915698468685\n",
            "310 10 lr - 0.000002 loss - 0.007973719388246536\n",
            "311 0 lr - 0.000002 loss - 0.0074134450405836105\n",
            "311 10 lr - 0.000002 loss - 0.005985991097986698\n",
            "312 0 lr - 0.000002 loss - 0.006634372286498547\n",
            "312 10 lr - 0.000002 loss - 0.006408443674445152\n",
            "313 0 lr - 0.000002 loss - 0.005941431503742933\n",
            "313 10 lr - 0.000002 loss - 0.007963381707668304\n",
            "314 0 lr - 0.000002 loss - 0.006897530518472195\n",
            "314 10 lr - 0.000002 loss - 0.00732465460896492\n",
            "315 0 lr - 0.000002 loss - 0.006503836717456579\n",
            "315 10 lr - 0.000002 loss - 0.008776193484663963\n",
            "316 0 lr - 0.000002 loss - 0.0069928839802742004\n",
            "316 10 lr - 0.000002 loss - 0.0074025532230734825\n",
            "317 0 lr - 0.000002 loss - 0.0064522563479840755\n",
            "317 10 lr - 0.000002 loss - 0.009004199877381325\n",
            "318 0 lr - 0.000002 loss - 0.006627699360251427\n",
            "318 10 lr - 0.000002 loss - 0.007328305393457413\n",
            "319 0 lr - 0.000002 loss - 0.007113066501915455\n",
            "319 10 lr - 0.000002 loss - 0.007227285765111446\n",
            "320 0 lr - 0.000002 loss - 0.006459835451096296\n",
            "320 10 lr - 0.000002 loss - 0.007724612019956112\n",
            "321 0 lr - 0.000001 loss - 0.0073353443294763565\n",
            "321 10 lr - 0.000001 loss - 0.008671602234244347\n",
            "322 0 lr - 0.000001 loss - 0.008578745648264885\n",
            "322 10 lr - 0.000001 loss - 0.007157298736274242\n",
            "323 0 lr - 0.000001 loss - 0.008132739923894405\n",
            "323 10 lr - 0.000001 loss - 0.008692927658557892\n",
            "324 0 lr - 0.000001 loss - 0.007351893000304699\n",
            "324 10 lr - 0.000001 loss - 0.006460720207542181\n",
            "325 0 lr - 0.000001 loss - 0.005454967729747295\n",
            "325 10 lr - 0.000001 loss - 0.006160744931548834\n",
            "326 0 lr - 0.000001 loss - 0.0078082941472530365\n",
            "326 10 lr - 0.000001 loss - 0.007004056125879288\n",
            "327 0 lr - 0.000001 loss - 0.007640692871063948\n",
            "327 10 lr - 0.000001 loss - 0.00712981354445219\n",
            "328 0 lr - 0.000001 loss - 0.006399425677955151\n",
            "328 10 lr - 0.000001 loss - 0.007972808554768562\n",
            "329 0 lr - 0.000001 loss - 0.00959183368831873\n",
            "329 10 lr - 0.000001 loss - 0.006118252407759428\n",
            "330 0 lr - 0.000001 loss - 0.007066015154123306\n",
            "330 10 lr - 0.000001 loss - 0.007718113251030445\n",
            "331 0 lr - 0.000001 loss - 0.006017966195940971\n",
            "331 10 lr - 0.000001 loss - 0.005729110445827246\n",
            "332 0 lr - 0.000001 loss - 0.0055806017480790615\n",
            "332 10 lr - 0.000001 loss - 0.009235106408596039\n",
            "333 0 lr - 0.000001 loss - 0.00567891588434577\n",
            "333 10 lr - 0.000001 loss - 0.005882154684513807\n",
            "334 0 lr - 0.000001 loss - 0.0058318316005170345\n",
            "334 10 lr - 0.000001 loss - 0.007071354892104864\n",
            "335 0 lr - 0.000001 loss - 0.0077761197462677956\n",
            "335 10 lr - 0.000001 loss - 0.009499277919530869\n",
            "336 0 lr - 0.000001 loss - 0.005834173411130905\n",
            "336 10 lr - 0.000001 loss - 0.004850524011999369\n",
            "337 0 lr - 0.000001 loss - 0.0071033453568816185\n",
            "337 10 lr - 0.000001 loss - 0.006618088111281395\n",
            "338 0 lr - 0.000001 loss - 0.007484043948352337\n",
            "338 10 lr - 0.000001 loss - 0.00825490616261959\n",
            "339 0 lr - 0.000001 loss - 0.0065099699422717094\n",
            "339 10 lr - 0.000001 loss - 0.008267540484666824\n",
            "340 0 lr - 0.000001 loss - 0.00774611160159111\n",
            "340 10 lr - 0.000001 loss - 0.009071601554751396\n",
            "341 0 lr - 0.000001 loss - 0.009694317355751991\n",
            "341 10 lr - 0.000001 loss - 0.007786180824041367\n",
            "342 0 lr - 0.000001 loss - 0.004488939419388771\n",
            "342 10 lr - 0.000001 loss - 0.007100225891917944\n",
            "343 0 lr - 0.000001 loss - 0.007576489355415106\n",
            "343 10 lr - 0.000001 loss - 0.005536993034183979\n",
            "344 0 lr - 0.000001 loss - 0.006693174596875906\n",
            "344 10 lr - 0.000001 loss - 0.0077656712383031845\n",
            "345 0 lr - 0.000001 loss - 0.00790519267320633\n",
            "345 10 lr - 0.000001 loss - 0.008650391362607479\n",
            "346 0 lr - 0.000001 loss - 0.006233492400497198\n",
            "346 10 lr - 0.000001 loss - 0.006631999742239714\n",
            "347 0 lr - 0.000001 loss - 0.006258510053157806\n",
            "347 10 lr - 0.000001 loss - 0.005911586806178093\n",
            "348 0 lr - 0.000001 loss - 0.006176239810883999\n",
            "348 10 lr - 0.000001 loss - 0.007288153283298016\n",
            "349 0 lr - 0.000001 loss - 0.007891692221164703\n",
            "349 10 lr - 0.000001 loss - 0.0072187925688922405\n",
            "350 0 lr - 0.000001 loss - 0.009122947230935097\n",
            "350 10 lr - 0.000001 loss - 0.005271982401609421\n",
            "351 0 lr - 0.000001 loss - 0.006410914473235607\n",
            "351 10 lr - 0.000001 loss - 0.008458133786916733\n",
            "352 0 lr - 0.000001 loss - 0.008575753308832645\n",
            "352 10 lr - 0.000001 loss - 0.008547495119273663\n",
            "353 0 lr - 0.000001 loss - 0.008676715195178986\n",
            "353 10 lr - 0.000001 loss - 0.005592239089310169\n",
            "354 0 lr - 0.000001 loss - 0.0077844224870204926\n",
            "354 10 lr - 0.000001 loss - 0.008096220903098583\n",
            "355 0 lr - 0.000001 loss - 0.008536595851182938\n",
            "355 10 lr - 0.000001 loss - 0.0068833972327411175\n",
            "356 0 lr - 0.000001 loss - 0.007508103735744953\n",
            "356 10 lr - 0.000001 loss - 0.007204156368970871\n",
            "357 0 lr - 0.000001 loss - 0.0047653596848249435\n",
            "357 10 lr - 0.000001 loss - 0.008134841918945312\n",
            "358 0 lr - 0.000001 loss - 0.006733113434165716\n",
            "358 10 lr - 0.000001 loss - 0.004942530766129494\n",
            "359 0 lr - 0.000001 loss - 0.0055701350793242455\n",
            "359 10 lr - 0.000001 loss - 0.006774872541427612\n",
            "360 0 lr - 0.000001 loss - 0.006578307598829269\n",
            "360 10 lr - 0.000001 loss - 0.005835351999849081\n",
            "361 0 lr - 0.000001 loss - 0.008411196991801262\n",
            "361 10 lr - 0.000001 loss - 0.006573255639523268\n",
            "362 0 lr - 0.000001 loss - 0.007259267382323742\n",
            "362 10 lr - 0.000001 loss - 0.006506315898150206\n",
            "363 0 lr - 0.000001 loss - 0.007978571578860283\n",
            "363 10 lr - 0.000001 loss - 0.005583095829933882\n",
            "364 0 lr - 0.000001 loss - 0.008919043466448784\n",
            "364 10 lr - 0.000001 loss - 0.008288800716400146\n",
            "365 0 lr - 0.000001 loss - 0.006223277188837528\n",
            "365 10 lr - 0.000001 loss - 0.007683722302317619\n",
            "366 0 lr - 0.000001 loss - 0.006451535504311323\n",
            "366 10 lr - 0.000001 loss - 0.007061116397380829\n",
            "367 0 lr - 0.000001 loss - 0.006216277834028006\n",
            "367 10 lr - 0.000001 loss - 0.008467525243759155\n",
            "368 0 lr - 0.000001 loss - 0.006877696141600609\n",
            "368 10 lr - 0.000001 loss - 0.010166235268115997\n",
            "369 0 lr - 0.000001 loss - 0.00620877742767334\n",
            "369 10 lr - 0.000001 loss - 0.006816589739173651\n",
            "370 0 lr - 0.000001 loss - 0.006867123767733574\n",
            "370 10 lr - 0.000001 loss - 0.006563541945070028\n",
            "371 0 lr - 0.000001 loss - 0.006425180938094854\n",
            "371 10 lr - 0.000001 loss - 0.008013634011149406\n",
            "372 0 lr - 0.000001 loss - 0.009064805693924427\n",
            "372 10 lr - 0.000001 loss - 0.0048165637999773026\n",
            "373 0 lr - 0.000001 loss - 0.007197511848062277\n",
            "373 10 lr - 0.000001 loss - 0.0084593016654253\n",
            "374 0 lr - 0.000001 loss - 0.006880949717015028\n",
            "374 10 lr - 0.000001 loss - 0.005251395981758833\n",
            "375 0 lr - 0.000001 loss - 0.007371389772742987\n",
            "375 10 lr - 0.000001 loss - 0.004970669746398926\n",
            "376 0 lr - 0.000000 loss - 0.00676461448892951\n",
            "376 10 lr - 0.000000 loss - 0.008979478850960732\n",
            "377 0 lr - 0.000000 loss - 0.005488823167979717\n",
            "377 10 lr - 0.000000 loss - 0.008034393191337585\n",
            "378 0 lr - 0.000000 loss - 0.009232766926288605\n",
            "378 10 lr - 0.000000 loss - 0.007956606335937977\n",
            "379 0 lr - 0.000000 loss - 0.008287087082862854\n",
            "379 10 lr - 0.000000 loss - 0.007602307014167309\n",
            "380 0 lr - 0.000000 loss - 0.006304738111793995\n",
            "380 10 lr - 0.000000 loss - 0.005981517489999533\n",
            "381 0 lr - 0.000000 loss - 0.007314725778996944\n",
            "381 10 lr - 0.000000 loss - 0.005765248090028763\n",
            "382 0 lr - 0.000000 loss - 0.00728661147877574\n",
            "382 10 lr - 0.000000 loss - 0.006290930323302746\n",
            "383 0 lr - 0.000000 loss - 0.00525830639526248\n",
            "383 10 lr - 0.000000 loss - 0.007159370929002762\n",
            "384 0 lr - 0.000000 loss - 0.008210439234972\n",
            "384 10 lr - 0.000000 loss - 0.008181791752576828\n",
            "385 0 lr - 0.000000 loss - 0.004577968269586563\n",
            "385 10 lr - 0.000000 loss - 0.007458328735083342\n",
            "386 0 lr - 0.000000 loss - 0.009342205710709095\n",
            "386 10 lr - 0.000000 loss - 0.0074966708198189735\n",
            "387 0 lr - 0.000000 loss - 0.007911646738648415\n",
            "387 10 lr - 0.000000 loss - 0.008748101070523262\n",
            "388 0 lr - 0.000000 loss - 0.0075524295680224895\n",
            "388 10 lr - 0.000000 loss - 0.008051235228776932\n",
            "389 0 lr - 0.000000 loss - 0.0058495900593698025\n",
            "389 10 lr - 0.000000 loss - 0.008492723107337952\n",
            "390 0 lr - 0.000000 loss - 0.005677772685885429\n",
            "390 10 lr - 0.000000 loss - 0.007072467356920242\n",
            "391 0 lr - 0.000000 loss - 0.004509718157351017\n",
            "391 10 lr - 0.000000 loss - 0.007972391322255135\n",
            "392 0 lr - 0.000000 loss - 0.005445535760372877\n",
            "392 10 lr - 0.000000 loss - 0.005491367541253567\n",
            "393 0 lr - 0.000000 loss - 0.006304300390183926\n",
            "393 10 lr - 0.000000 loss - 0.006708439439535141\n",
            "394 0 lr - 0.000000 loss - 0.00792626105248928\n",
            "394 10 lr - 0.000000 loss - 0.007010642439126968\n",
            "395 0 lr - 0.000000 loss - 0.009745175018906593\n",
            "395 10 lr - 0.000000 loss - 0.0057402378879487514\n",
            "396 0 lr - 0.000000 loss - 0.007830098271369934\n",
            "396 10 lr - 0.000000 loss - 0.005410654004663229\n",
            "397 0 lr - 0.000000 loss - 0.007119470741599798\n",
            "397 10 lr - 0.000000 loss - 0.0058434149250388145\n",
            "398 0 lr - 0.000000 loss - 0.007257263176143169\n",
            "398 10 lr - 0.000000 loss - 0.008304748684167862\n",
            "399 0 lr - 0.000000 loss - 0.004569396376609802\n",
            "399 10 lr - 0.000000 loss - 0.00866079144179821\n",
            "400 0 lr - 0.000000 loss - 0.006830568891018629\n",
            "400 10 lr - 0.000000 loss - 0.005388995632529259\n",
            "401 0 lr - 0.000000 loss - 0.009584031999111176\n",
            "401 10 lr - 0.000000 loss - 0.00707900058478117\n",
            "402 0 lr - 0.000000 loss - 0.005678347311913967\n",
            "402 10 lr - 0.000000 loss - 0.008593041449785233\n",
            "403 0 lr - 0.000000 loss - 0.006195375230163336\n",
            "403 10 lr - 0.000000 loss - 0.008940133266150951\n",
            "404 0 lr - 0.000000 loss - 0.009318393655121326\n",
            "404 10 lr - 0.000000 loss - 0.007996713742613792\n",
            "405 0 lr - 0.000000 loss - 0.005718853324651718\n",
            "405 10 lr - 0.000000 loss - 0.00644730543717742\n",
            "406 0 lr - 0.000000 loss - 0.008349359035491943\n",
            "406 10 lr - 0.000000 loss - 0.006715934723615646\n",
            "407 0 lr - 0.000000 loss - 0.006874389480799437\n",
            "407 10 lr - 0.000000 loss - 0.009958978742361069\n",
            "408 0 lr - 0.000000 loss - 0.0069255344569683075\n",
            "408 10 lr - 0.000000 loss - 0.007007082458585501\n",
            "409 0 lr - 0.000000 loss - 0.006123857107013464\n",
            "409 10 lr - 0.000000 loss - 0.005757870152592659\n",
            "410 0 lr - 0.000000 loss - 0.009173791855573654\n",
            "410 10 lr - 0.000000 loss - 0.0047436547465622425\n",
            "411 0 lr - 0.000000 loss - 0.006548787467181683\n",
            "411 10 lr - 0.000000 loss - 0.005961775314062834\n",
            "412 0 lr - 0.000000 loss - 0.004502419847995043\n",
            "412 10 lr - 0.000000 loss - 0.006361761596053839\n",
            "413 0 lr - 0.000000 loss - 0.005522688385099173\n",
            "413 10 lr - 0.000000 loss - 0.008887536823749542\n",
            "414 0 lr - 0.000000 loss - 0.0071843452751636505\n",
            "414 10 lr - 0.000000 loss - 0.008866212330758572\n",
            "415 0 lr - 0.000000 loss - 0.007907988503575325\n",
            "415 10 lr - 0.000000 loss - 0.006031312979757786\n",
            "416 0 lr - 0.000000 loss - 0.008203214965760708\n",
            "416 10 lr - 0.000000 loss - 0.007054655812680721\n",
            "417 0 lr - 0.000000 loss - 0.0070898025296628475\n",
            "417 10 lr - 0.000000 loss - 0.008026838302612305\n",
            "418 0 lr - 0.000000 loss - 0.006292172707617283\n",
            "418 10 lr - 0.000000 loss - 0.00835468526929617\n",
            "419 0 lr - 0.000000 loss - 0.008984625339508057\n",
            "419 10 lr - 0.000000 loss - 0.0065145594999194145\n",
            "420 0 lr - 0.000000 loss - 0.008119003847241402\n",
            "420 10 lr - 0.000000 loss - 0.007893253117799759\n",
            "421 0 lr - 0.000000 loss - 0.006440249737352133\n",
            "421 10 lr - 0.000000 loss - 0.005374297499656677\n",
            "422 0 lr - 0.000000 loss - 0.0067623755894601345\n",
            "422 10 lr - 0.000000 loss - 0.0063922349363565445\n",
            "423 0 lr - 0.000000 loss - 0.0069702304899692535\n",
            "423 10 lr - 0.000000 loss - 0.009075040929019451\n",
            "424 0 lr - 0.000000 loss - 0.0058586313389241695\n",
            "424 10 lr - 0.000000 loss - 0.00773052079603076\n",
            "425 0 lr - 0.000000 loss - 0.008878534659743309\n",
            "425 10 lr - 0.000000 loss - 0.006632216274738312\n",
            "426 0 lr - 0.000000 loss - 0.0073021044954657555\n",
            "426 10 lr - 0.000000 loss - 0.008405573666095734\n",
            "427 0 lr - 0.000000 loss - 0.007030585315078497\n",
            "427 10 lr - 0.000000 loss - 0.00624570157378912\n",
            "428 0 lr - 0.000000 loss - 0.00860885065048933\n",
            "428 10 lr - 0.000000 loss - 0.007394022308290005\n",
            "429 0 lr - 0.000000 loss - 0.007136333733797073\n",
            "429 10 lr - 0.000000 loss - 0.00930537935346365\n",
            "430 0 lr - 0.000000 loss - 0.0075418585911393166\n",
            "430 10 lr - 0.000000 loss - 0.007316619157791138\n",
            "431 0 lr - 0.000000 loss - 0.0065305037423968315\n",
            "431 10 lr - 0.000000 loss - 0.006448843516409397\n",
            "432 0 lr - 0.000000 loss - 0.008761804550886154\n",
            "432 10 lr - 0.000000 loss - 0.007539740763604641\n",
            "433 0 lr - 0.000000 loss - 0.006476380862295628\n",
            "433 10 lr - 0.000000 loss - 0.008085409179329872\n",
            "434 0 lr - 0.000000 loss - 0.008090241812169552\n",
            "434 10 lr - 0.000000 loss - 0.006708237342536449\n",
            "435 0 lr - 0.000000 loss - 0.009186056442558765\n",
            "435 10 lr - 0.000000 loss - 0.00781412236392498\n",
            "436 0 lr - 0.000000 loss - 0.0070564462803304195\n",
            "436 10 lr - 0.000000 loss - 0.006391131319105625\n",
            "437 0 lr - 0.000000 loss - 0.004402870312333107\n",
            "437 10 lr - 0.000000 loss - 0.005808833986520767\n",
            "438 0 lr - 0.000000 loss - 0.007558449637144804\n",
            "438 10 lr - 0.000000 loss - 0.007184553425759077\n",
            "439 0 lr - 0.000000 loss - 0.005384994205087423\n",
            "439 10 lr - 0.000000 loss - 0.008829586207866669\n",
            "440 0 lr - 0.000000 loss - 0.009653675369918346\n",
            "440 10 lr - 0.000000 loss - 0.009626565501093864\n",
            "441 0 lr - 0.000000 loss - 0.009204494766891003\n",
            "441 10 lr - 0.000000 loss - 0.0082752276211977\n",
            "442 0 lr - 0.000000 loss - 0.009808739647269249\n",
            "442 10 lr - 0.000000 loss - 0.007926822640001774\n",
            "443 0 lr - 0.000000 loss - 0.00809837132692337\n",
            "443 10 lr - 0.000000 loss - 0.005439679604023695\n",
            "444 0 lr - 0.000000 loss - 0.006978805176913738\n",
            "444 10 lr - 0.000000 loss - 0.006301030516624451\n",
            "445 0 lr - 0.000000 loss - 0.007612201385200024\n",
            "445 10 lr - 0.000000 loss - 0.006343034561723471\n",
            "446 0 lr - 0.000000 loss - 0.00674823485314846\n",
            "446 10 lr - 0.000000 loss - 0.005377276800572872\n",
            "447 0 lr - 0.000000 loss - 0.005869336426258087\n",
            "447 10 lr - 0.000000 loss - 0.008549073711037636\n",
            "448 0 lr - 0.000000 loss - 0.007246983237564564\n",
            "448 10 lr - 0.000000 loss - 0.007441913243383169\n",
            "449 0 lr - 0.000000 loss - 0.00624336302280426\n",
            "449 10 lr - 0.000000 loss - 0.007416774518787861\n",
            "450 0 lr - 0.000000 loss - 0.007377792149782181\n",
            "450 10 lr - 0.000000 loss - 0.008405650034546852\n",
            "451 0 lr - 0.000000 loss - 0.00869322381913662\n",
            "451 10 lr - 0.000000 loss - 0.0071483696810901165\n",
            "452 0 lr - 0.000000 loss - 0.006817694753408432\n",
            "452 10 lr - 0.000000 loss - 0.007798759266734123\n",
            "453 0 lr - 0.000000 loss - 0.007756859064102173\n",
            "453 10 lr - 0.000000 loss - 0.0070498231798410416\n",
            "454 0 lr - 0.000000 loss - 0.004711739718914032\n",
            "454 10 lr - 0.000000 loss - 0.0072477394714951515\n",
            "455 0 lr - 0.000000 loss - 0.008616220206022263\n",
            "455 10 lr - 0.000000 loss - 0.006222823169082403\n",
            "456 0 lr - 0.000000 loss - 0.00760638527572155\n",
            "456 10 lr - 0.000000 loss - 0.007869591936469078\n",
            "457 0 lr - 0.000000 loss - 0.004648164846003056\n",
            "457 10 lr - 0.000000 loss - 0.00886739231646061\n",
            "458 0 lr - 0.000000 loss - 0.007172899320721626\n",
            "458 10 lr - 0.000000 loss - 0.007525581866502762\n",
            "459 0 lr - 0.000000 loss - 0.004960787016898394\n",
            "459 10 lr - 0.000000 loss - 0.007887901738286018\n",
            "460 0 lr - 0.000000 loss - 0.008114267140626907\n",
            "460 10 lr - 0.000000 loss - 0.006889007985591888\n",
            "461 0 lr - 0.000000 loss - 0.008691552095115185\n",
            "461 10 lr - 0.000000 loss - 0.006867941468954086\n",
            "462 0 lr - 0.000000 loss - 0.007230173796415329\n",
            "462 10 lr - 0.000000 loss - 0.007031520362943411\n",
            "463 0 lr - 0.000000 loss - 0.007179914973676205\n",
            "463 10 lr - 0.000000 loss - 0.004985467996448278\n",
            "464 0 lr - 0.000000 loss - 0.007768306415528059\n",
            "464 10 lr - 0.000000 loss - 0.007619332522153854\n",
            "465 0 lr - 0.000000 loss - 0.00786086730659008\n",
            "465 10 lr - 0.000000 loss - 0.0069481441751122475\n",
            "466 0 lr - 0.000000 loss - 0.006328899413347244\n",
            "466 10 lr - 0.000000 loss - 0.006227071397006512\n",
            "467 0 lr - 0.000000 loss - 0.009313227608799934\n",
            "467 10 lr - 0.000000 loss - 0.006108961999416351\n",
            "468 0 lr - 0.000000 loss - 0.007551506161689758\n",
            "468 10 lr - 0.000000 loss - 0.008773071691393852\n",
            "469 0 lr - 0.000000 loss - 0.006916874088346958\n",
            "469 10 lr - 0.000000 loss - 0.007451406680047512\n",
            "470 0 lr - 0.000000 loss - 0.007176544051617384\n",
            "470 10 lr - 0.000000 loss - 0.007659118622541428\n",
            "471 0 lr - 0.000000 loss - 0.007884612306952477\n",
            "471 10 lr - 0.000000 loss - 0.007906047627329826\n",
            "472 0 lr - 0.000000 loss - 0.0075614359229803085\n",
            "472 10 lr - 0.000000 loss - 0.006997551769018173\n",
            "473 0 lr - 0.000000 loss - 0.005768997129052877\n",
            "473 10 lr - 0.000000 loss - 0.007137373089790344\n",
            "474 0 lr - 0.000000 loss - 0.0048270258121192455\n",
            "474 10 lr - 0.000000 loss - 0.00564709585160017\n",
            "475 0 lr - 0.000000 loss - 0.008238330483436584\n",
            "475 10 lr - 0.000000 loss - 0.00872963946312666\n",
            "476 0 lr - 0.000000 loss - 0.007509138435125351\n",
            "476 10 lr - 0.000000 loss - 0.007010740227997303\n",
            "477 0 lr - 0.000000 loss - 0.008461627177894115\n",
            "477 10 lr - 0.000000 loss - 0.008222479373216629\n",
            "478 0 lr - 0.000000 loss - 0.007199530955404043\n",
            "478 10 lr - 0.000000 loss - 0.007618211209774017\n",
            "479 0 lr - 0.000000 loss - 0.006895638536661863\n",
            "479 10 lr - 0.000000 loss - 0.005593674257397652\n",
            "480 0 lr - 0.000000 loss - 0.007397592067718506\n",
            "480 10 lr - 0.000000 loss - 0.007355549838393927\n",
            "481 0 lr - 0.000000 loss - 0.005710676312446594\n",
            "481 10 lr - 0.000000 loss - 0.007983841933310032\n",
            "482 0 lr - 0.000000 loss - 0.00797958578914404\n",
            "482 10 lr - 0.000000 loss - 0.004828396253287792\n",
            "483 0 lr - 0.000000 loss - 0.007419902365654707\n",
            "483 10 lr - 0.000000 loss - 0.006755865179002285\n",
            "484 0 lr - 0.000000 loss - 0.007060777395963669\n",
            "484 10 lr - 0.000000 loss - 0.007343068718910217\n",
            "485 0 lr - 0.000000 loss - 0.007953325286507607\n",
            "485 10 lr - 0.000000 loss - 0.007134684827178717\n",
            "486 0 lr - 0.000000 loss - 0.007168098818510771\n",
            "486 10 lr - 0.000000 loss - 0.01025020144879818\n",
            "487 0 lr - 0.000000 loss - 0.007991357706487179\n",
            "487 10 lr - 0.000000 loss - 0.008739418350160122\n",
            "488 0 lr - 0.000000 loss - 0.009441309608519077\n",
            "488 10 lr - 0.000000 loss - 0.007515713572502136\n",
            "489 0 lr - 0.000000 loss - 0.008860789239406586\n",
            "489 10 lr - 0.000000 loss - 0.0062053329311311245\n",
            "490 0 lr - 0.000000 loss - 0.0075215911492705345\n",
            "490 10 lr - 0.000000 loss - 0.005768525414168835\n",
            "491 0 lr - 0.000000 loss - 0.009317050687968731\n",
            "491 10 lr - 0.000000 loss - 0.007177971303462982\n",
            "492 0 lr - 0.000000 loss - 0.007753070443868637\n",
            "492 10 lr - 0.000000 loss - 0.005865998566150665\n",
            "493 0 lr - 0.000000 loss - 0.007362996228039265\n",
            "493 10 lr - 0.000000 loss - 0.007508771028369665\n",
            "494 0 lr - 0.000000 loss - 0.006157787051051855\n",
            "494 10 lr - 0.000000 loss - 0.007148980163037777\n",
            "495 0 lr - 0.000000 loss - 0.007331369444727898\n",
            "495 10 lr - 0.000000 loss - 0.0075978245586156845\n",
            "496 0 lr - 0.000000 loss - 0.006410523317754269\n",
            "496 10 lr - 0.000000 loss - 0.005938676651567221\n",
            "497 0 lr - 0.000000 loss - 0.007255277596414089\n",
            "497 10 lr - 0.000000 loss - 0.007422413676977158\n",
            "498 0 lr - 0.000000 loss - 0.008074335753917694\n",
            "498 10 lr - 0.000000 loss - 0.009648049250245094\n",
            "499 0 lr - 0.000000 loss - 0.006459768861532211\n",
            "499 10 lr - 0.000000 loss - 0.006302855908870697\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 361... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1b9ba0e532fe4a6892577c8d0126913f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value=' 28094.86MB of 28094.86MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
              "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█▆▅▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>temperature</td><td>██████▇▇▇▇▆▆▆▅▅▄▄▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
              "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>499</td></tr><tr><td>iter</td><td>10</td></tr><tr><td>loss</td><td>0.0063</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>temperature</td><td>0.5</td></tr></table>\n",
              "</div></div>\n",
              "Synced 5 W&B file(s), 1500 media file(s), 501 artifact file(s) and 2 other file(s)\n",
              "<br/>Synced <strong style=\"color:#cdcd00\">stilted-smoke-1</strong>: <a href=\"https://wandb.ai/gavincapriola/dalle_train_vae/runs/2ogtsahn\" target=\"_blank\">https://wandb.ai/gavincapriola/dalle_train_vae/runs/2ogtsahn</a><br/>\n",
              "Find logs at: <code>./wandb/run-20220204_214121-2ogtsahn/logs</code><br/>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import math\n",
        "from math import sqrt\n",
        "\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "# vision imports\n",
        "from torchvision import transforms as T\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.utils import make_grid, save_image\n",
        "\n",
        "# dalle classes\n",
        "from dalle_pytorch import DiscreteVAE\n",
        "\n",
        "# constants\n",
        "IMAGE_SIZE = 128\n",
        "IMAGE_PATH = '/content/botpeg/images'\n",
        "\n",
        "EPOCHS = 500\n",
        "BATCH_SIZE = 4\n",
        "LEARNING_RATE = 1e-3\n",
        "LR_DECAY_RATE = 0.98\n",
        "\n",
        "NUM_TOKENS = 8192\n",
        "NUM_LAYERS = 2\n",
        "NUM_RESNET_BLOCKS = 2\n",
        "SMOOTH_L1_LOSS = False\n",
        "EMB_DIM = 512\n",
        "HID_DIM = 256\n",
        "KL_LOSS_WEIGHT = 0\n",
        "\n",
        "STARTING_TEMP = 1.\n",
        "TEMP_MIN = 0.5\n",
        "ANNEAL_RATE = 1e-6\n",
        "\n",
        "NUM_IMAGES_SAVE = 4\n",
        "\n",
        "# data\n",
        "ds = ImageFolder(\n",
        "    IMAGE_PATH,\n",
        "    T.Compose([\n",
        "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
        "        T.Resize(IMAGE_SIZE),\n",
        "        T.CenterCrop(IMAGE_SIZE),\n",
        "        T.ToTensor()\n",
        "    ])\n",
        ")\n",
        "\n",
        "dl = DataLoader(ds, BATCH_SIZE, shuffle = True)\n",
        "\n",
        "vae_params = dict(\n",
        "    image_size = IMAGE_SIZE,\n",
        "    num_layers = NUM_LAYERS,\n",
        "    num_tokens = NUM_TOKENS,\n",
        "    codebook_dim = EMB_DIM,\n",
        "    hidden_dim   = HID_DIM,\n",
        "    num_resnet_blocks = NUM_RESNET_BLOCKS\n",
        ")\n",
        "\n",
        "vae = DiscreteVAE(\n",
        "    **vae_params,\n",
        "    smooth_l1_loss = SMOOTH_L1_LOSS,\n",
        "    kl_div_loss_weight = KL_LOSS_WEIGHT\n",
        ").cuda()\n",
        "\n",
        "\n",
        "assert len(ds) > 0, 'folder does not contain any images'\n",
        "print(f'{len(ds)} images found for training')\n",
        "\n",
        "def save_model(path):\n",
        "    save_obj = {\n",
        "        'hparams': vae_params,\n",
        "        'weights': vae.state_dict()\n",
        "    }\n",
        "\n",
        "    torch.save(save_obj, path)\n",
        "\n",
        "# optimizer\n",
        "opt = Adam(vae.parameters(), lr = LEARNING_RATE)\n",
        "sched = ExponentialLR(optimizer = opt, gamma = LR_DECAY_RATE)\n",
        "\n",
        "# weights & biases experiment tracking\n",
        "model_config = dict(\n",
        "    num_tokens = NUM_TOKENS,\n",
        "    smooth_l1_loss = SMOOTH_L1_LOSS,\n",
        "    num_resnet_blocks = NUM_RESNET_BLOCKS,\n",
        "    kl_loss_weight = KL_LOSS_WEIGHT\n",
        ")\n",
        "\n",
        "run = wandb.init(\n",
        "    project = 'dalle_train_vae',\n",
        "    job_type = 'train_model',\n",
        "    config = model_config\n",
        ")\n",
        "\n",
        "# starting temperature\n",
        "global_step = 0\n",
        "temp = STARTING_TEMP\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    for i, (images, _) in enumerate(dl):\n",
        "        images = images.cuda()\n",
        "\n",
        "        loss, recons = vae(\n",
        "            images,\n",
        "            return_loss = True,\n",
        "            return_recons = True,\n",
        "            temp = temp\n",
        "        )\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        logs = {}\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            k = NUM_IMAGES_SAVE\n",
        "\n",
        "            with torch.no_grad():\n",
        "                codes = vae.get_codebook_indices(images[:k])\n",
        "                hard_recons = vae.decode(codes)\n",
        "\n",
        "            images, recons = map(lambda t: t[:k], (images, recons))\n",
        "            images, recons, hard_recons, codes = map(lambda t: t.detach().cpu(), (images, recons, hard_recons, codes))\n",
        "            images, recons, hard_recons = map(lambda t: make_grid(t.float(), nrow = int(sqrt(k)), normalize = True, range = (-1, 1)), (images, recons, hard_recons))\n",
        "\n",
        "            logs = {\n",
        "                **logs,\n",
        "                'sample images':        wandb.Image(images, caption = 'original images'),\n",
        "                'reconstructions':      wandb.Image(recons, caption = 'reconstructions'),\n",
        "                'hard reconstructions': wandb.Image(hard_recons, caption = 'hard reconstructions'),\n",
        "                'codebook_indices':     wandb.Histogram(codes),\n",
        "                'temperature':          temp\n",
        "            }\n",
        "\n",
        "            save_model(f'./vae.pt')\n",
        "            wandb.save('./vae.pt')\n",
        "\n",
        "            # temperature anneal\n",
        "            temp = max(temp * math.exp(-ANNEAL_RATE * global_step), TEMP_MIN)\n",
        "\n",
        "            # lr decay\n",
        "            sched.step()\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            lr = sched.get_last_lr()[0]\n",
        "            print(epoch, i, f'lr - {lr:6f} loss - {loss.item()}')\n",
        "\n",
        "            logs = {\n",
        "                **logs,\n",
        "                'epoch': epoch,\n",
        "                'iter': i,\n",
        "                'loss': loss.item(),\n",
        "                'lr': lr\n",
        "            }\n",
        "\n",
        "        wandb.log(logs)\n",
        "        global_step += 1\n",
        "\n",
        "    # save trained model to wandb as an artifact every epoch's end\n",
        "    model_artifact = wandb.Artifact('trained-vae', type = 'model', metadata = dict(model_config))\n",
        "    model_artifact.add_file('vae.pt')\n",
        "    run.log_artifact(model_artifact)\n",
        "\n",
        "# save final vae and cleanup\n",
        "save_model('./vae-final.pt')\n",
        "wandb.save('./vae-final.pt')\n",
        "\n",
        "model_artifact = wandb.Artifact('trained-vae', type = 'model', metadata = dict(model_config))\n",
        "model_artifact.add_file('vae-final.pt')\n",
        "run.log_artifact(model_artifact)\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LepHGyR2wFa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "train-vae.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1b9ba0e532fe4a6892577c8d0126913f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b72586872a494d2bb854c68c8a7f3e53",
              "IPY_MODEL_5b67e3881873459cac132e46d20a4bdf"
            ],
            "layout": "IPY_MODEL_9920c754ca4540f7b1f15ba82d075aaa"
          }
        },
        "4c927221d8e04023b7d47237cb69daa5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59324b4498e544ad8c3686e2e6c9cabc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b67e3881873459cac132e46d20a4bdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c927221d8e04023b7d47237cb69daa5",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d43c03c1d0754be1a1858eac01b9f18f",
            "value": 1
          }
        },
        "9920c754ca4540f7b1f15ba82d075aaa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b72586872a494d2bb854c68c8a7f3e53": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59324b4498e544ad8c3686e2e6c9cabc",
            "placeholder": "​",
            "style": "IPY_MODEL_ec7916cdc9ef4db59748cdafde9492ef",
            "value": " 28094.91MB of 28094.91MB uploaded (0.00MB deduped)\r"
          }
        },
        "d43c03c1d0754be1a1858eac01b9f18f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ec7916cdc9ef4db59748cdafde9492ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
